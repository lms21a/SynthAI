{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0f1b250-2c1b-43b5-98f3-ae0a4d5bcac5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Gzip Funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9812ea-d13c-4935-a777-2a6294aa5814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import subprocess\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "folder_dir = '/home/lukas928/Synth2/SynthAI/test_folder/'\n",
    "\n",
    "def compress_folder(directory):\n",
    "     # Get a list of all files in the directory\n",
    "    files_in_directory = os.listdir(directory)\n",
    "    \n",
    "    # Iterate over each file\n",
    "    for filename in tqdm(files_in_directory,desc='Compressing Files'):\n",
    "        # Construct full file path\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        # Open the original file\n",
    "        with open(file_path, 'rb') as f_in:\n",
    "            # Construct the name of the compressed file\n",
    "            compressed_file_path = file_path + '.gz'\n",
    "            \n",
    "            # Open the compressed file\n",
    "            with gzip.open(compressed_file_path, 'wb') as f_out:\n",
    "                # Copy the contents of the original file to the compressed file\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "        \n",
    "        # Delete the original file\n",
    "        os.remove(file_path)\n",
    "    \n",
    "    print(\"All files in the directory have been compressed and the original files have been deleted.\")\n",
    "            \n",
    "def decompress_folder(directory):\n",
    "# Get a list of all files in the directory\n",
    "    files_in_directory = os.listdir(directory)\n",
    "    \n",
    "    # Iterate over each file\n",
    "    for filename in tqdm(files_in_directory,desc='Decompressing Files'):\n",
    "        # Check if the file is a gzip file\n",
    "        if filename.endswith('.gz'):\n",
    "            # Construct full file path\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            \n",
    "            # Open the gzip file\n",
    "            with gzip.open(file_path, 'rb') as f_in:\n",
    "                # Construct the name of the decompressed file\n",
    "                decompressed_file_path = file_path[:-3]\n",
    "                \n",
    "                # Open the decompressed file\n",
    "                with open(decompressed_file_path, 'wb') as f_out:\n",
    "                    # Copy the contents of the gzip file to the decompressed file\n",
    "                    shutil.copyfileobj(f_in, f_out)\n",
    "            \n",
    "            # Delete the original gzip file\n",
    "            os.remove(file_path)\n",
    "    \n",
    "    print(\"All gzip files in the directory have been decompressed and deleted.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107b7105-d262-4d01-9c15-24b5952cc73e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38f7dd8c-dbd1-4d01-97e6-d6566b802614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/lukas928/Synth2/SynthAI/src'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c4349fe-2d00-41d1-ba4e-a11541cd7d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the tiny shakespeare dataset\n",
    "def create_file(file_dir, file_name):\n",
    "    if not os.path.exists(file_dir):\n",
    "        os.makedirs(file_dir)\n",
    "    return os.path.join(file_dir, file_name)\n",
    "    \n",
    "input_file_path = create_file('/home/lukas928/Synth2/SynthAI/data','shakespeare.txt')\n",
    "if not os.path.exists(input_file_path):\n",
    "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "    with open(input_file_path, 'w') as f:\n",
    "        f.write(requests.get(data_url).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a18554a-c5b3-438d-b4bd-57edee45a0fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(input_file_path, 'r') as f:\n",
    "    data = f.read()\n",
    "\n",
    "data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d975d57c-78c5-4a31-ab93-cf075d1592bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "338026"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = tiktoken.encoding_for_model('gpt2')\n",
    "tokens = enc.encode(data)\n",
    "tokens.append(enc.eot_token)\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d5ea2574-4f44-4b19-aad9-b039641d1f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = np.array(tokens)\n",
    "splits = np.array_split(tokens,10)\n",
    "for i in range(len(splits)):\n",
    "    np.save(f'/home/lukas928/Synth2/SynthAI/test_folder/t_{i}.tokens',np.array(splits[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d18be21-6ae1-4b6b-81bf-1dfc083658c9",
   "metadata": {},
   "source": [
    "# Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ab7a65d-c3b0-4721-b0bc-c72ae8d192e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5962, 22307, 25, 198, 8421, 356, 5120, 597, 2252, 11]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "def create_file(file_dir, file_name):\n",
    "    if not os.path.exists(file_dir):\n",
    "        os.makedirs(file_dir)\n",
    "    return os.path.join(file_dir, file_name)\n",
    "    \n",
    "input_file_path = create_file('/home/lukas928/Synth2/SynthAI/data','shakespeare.txt')\n",
    "\n",
    "with open(input_file_path, 'r') as f:\n",
    "    data = f.read()\n",
    "\n",
    "enc = tiktoken.encoding_for_model('gpt2')\n",
    "tokens = enc.encode_ordinary(data)\n",
    "tokens.append(enc.eot_token)\n",
    "tokens[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3753c51f-5644-4c48-9a2e-0157130b036a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 8]), torch.Size([4, 8]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a batch of data\n",
    "train = tokens[:int(.8*len(tokens))]\n",
    "test = tokens[int(.8*len(tokens)):]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def get_batch(split,btch,cntx):\n",
    "    data = train if split =='train' else test \n",
    "    len_data = len(data)-cntx  \n",
    "\n",
    "    # Generate all starting indices at once\n",
    "    start_indices = torch.randint(len_data, (btch,)).tolist()\n",
    "\n",
    "    x = torch.tensor(np.array([data[i:i+cntx] for i in start_indices])) #.pin_memory().to(device, non_blocking=True)\n",
    "    y = torch.tensor(np.array([data[i+1:i+cntx+1] for i in start_indices]))#.pin_memory().to(device, non_blocking=True)\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "x,y = get_batch('train',4,8)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ff54aac-4f0d-462e-be5d-7187219677e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "class Wte_Wpe(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, cntx_len,dropout=0.0):\n",
    "        super(Wte_Wpe, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.cntx_len = cntx_len\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout_p = dropout\n",
    "        self.wte = nn.Embedding(vocab_size, d_model)\n",
    "        self.wpe = nn.Embedding(cntx_len, d_model)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        nn.init.normal_(self.wte.weight, std=0.02)\n",
    "        nn.init.normal_(self.wpe.weight, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pos = torch.arange(0, self.cntx_len,dtype=torch.long, device=x.device)\n",
    "        return F.dropout(self.wte(x)+self.wpe(pos),p=self.dropout_p)\n",
    "    \n",
    "\n",
    "class CSA_torch(nn.Module):\n",
    "    def __init__(self, d_model, n_head,dropout=0.0):\n",
    "        super(CSA_torch, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_head = n_head\n",
    "        assert d_model % n_head == 0\n",
    "        self.head_size = d_model // n_head\n",
    "        self.dropout_p = dropout\n",
    "\n",
    "        self.qkv = nn.Linear(d_model, 3*d_model)\n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "        self.init_weights()\n",
    "    def init_weights(self):\n",
    "            nn.init.xavier_uniform_(self.qkv.weight)\n",
    "            nn.init.xavier_uniform_(self.fc_out.weight)\n",
    "            nn.init.zeros_(self.qkv.bias)\n",
    "            nn.init.zeros_(self.fc_out.bias)\n",
    "    def forward(self, x):\n",
    "        q,k,v = self.qkv(x).split(self.d_model, dim=2)\n",
    "        q = rearrange(q, 'b t (nh hs) -> b nh t hs', nh=self.n_head, hs=self.head_size)\n",
    "        k = rearrange(k, 'b t (nh hs) -> b nh t hs', nh=self.n_head, hs=self.head_size)\n",
    "        v = rearrange(v, 'b t (nh hs) -> b nh t hs', nh=self.n_head, hs=self.head_size)\n",
    "        y = F.scaled_dot_product_attention(q, k, v,\n",
    "                                           dropout_p=self.dropout_p, is_causal=True)\n",
    "        y = rearrange(y, 'b nh t hs -> b t (nh hs)')\n",
    "        return F.dropout(self.fc_out(y), p=self.dropout_p)\n",
    "\n",
    "class GPT_Block_torch(nn.Module):\n",
    "    def __init__(self, d_model, n_head, dropout=0.0):\n",
    "        super(GPT_Block_torch, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_head = n_head\n",
    "        self.dropout_p = dropout\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.csa = CSA_torch(d_model,n_head,dropout)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, 4*d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4*d_model, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        for module in self.mlp:\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.csa(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPT_Config:\n",
    "    vocab_size: int = 50257\n",
    "    d_model: int = 2048\n",
    "    cntx_len: int = 32 \n",
    "    n_layers: int = 15 \n",
    "    n_head: int = 64 \n",
    "    dropout_p: float = 0.0\n",
    "\n",
    "class GPT_torch(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(GPT_torch, self).__init__()\n",
    "        self.config = config\n",
    "        self.wte_wpe = Wte_Wpe(config.vocab_size, config.d_model, config.cntx_len, config.dropout_p)\n",
    "        self.blocks = nn.ModuleList([GPT_Block_torch(config.d_model, config.n_head, config.dropout_p) for _ in range(config.n_layers)])\n",
    "        self.ln = nn.LayerNorm(config.d_model)\n",
    "        self.fc_out = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    " \n",
    "    def forward(self,x,y=None):\n",
    "        x = self.wte_wpe(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        logits = self.fc_out(self.ln(x))\n",
    "        if y is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1,logits.size(-1)), y.view(-1))\n",
    "            return loss, logits\n",
    "        return logits\n",
    "    \n",
    "    def print_model_size(self):\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        formatted_size = \"{:,}\".format(total_params)\n",
    "        print(f\"Model size: {formatted_size} parameters\")\n",
    "    \n",
    "    def count_model_memory(self):\n",
    "        total_memory = 0\n",
    "        for param in self.parameters():\n",
    "            # Multiply number of elements in tensor by its byte size\n",
    "            total_memory += param.nelement() * param.element_size()\n",
    "\n",
    "        # Convert bytes to megabytes\n",
    "        total_memory_mb = total_memory / (1024 ** 2)\n",
    "\n",
    "        # Get total GPU memory and used memory\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        total_gpu_memory_mb = torch.cuda.get_device_properties(device).total_memory / (1024 ** 2)\n",
    "        current_gpu_memory_mb = torch.cuda.memory_allocated(device) / (1024 ** 2)\n",
    "\n",
    "        print(f\"Model memory usage: {total_memory_mb:.2f} MB\")\n",
    "        print(f\"Current GPU memory usage: {current_gpu_memory_mb:.2f} MB / {total_gpu_memory_mb:.2f} MB\")\n",
    "    \n",
    "\n",
    "    # https://github.com/karpathy/nanoGPT/blob/master/model.py\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at cntx_len\n",
    "            idx_cond = idx if idx.size(1) <= self.config.cntx_len else idx[:, -self.config.cntx_len:]\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits = self(idx_cond)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5007a68f-7918-481e-acb0-1c0cca609707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_sqrt_scheduler(step, num_warmup_steps, scale=.01, print_lr=False):\n",
    "    if step == 0:\n",
    "        lr = .05 if num_warmup_steps == 0 else 0\n",
    "    elif step < num_warmup_steps:\n",
    "        lr = float(step) / float(num_warmup_steps)\n",
    "    else:\n",
    "        lr = (1. / math.sqrt(step)) * scale\n",
    "\n",
    "    if print_lr: \n",
    "        print(lr)\n",
    "\n",
    "    return lr\n",
    "\n",
    "def adaptive_momentum_scheduler(grad, lr, momentum, momentum_decay=0.9, lr_min=1e-5, lr_max=0.1):\n",
    "    # Update the momentum\n",
    "    momentum = momentum_decay * momentum + (1 - momentum_decay) * grad\n",
    "\n",
    "    # Adjust the learning rate based on the momentum\n",
    "    if momentum > 0:\n",
    "        # If the momentum is high, decrease the learning rate\n",
    "        lr /= (1 + momentum)\n",
    "    else:\n",
    "        # If the momentum is low, increase the learning rate\n",
    "        lr *= (1 - momentum)\n",
    "\n",
    "    # Clip the learning rate to be within [lr_min, lr_max]\n",
    "    lr = max(lr_min, min(lr, lr_max))\n",
    "\n",
    "    return lr, momentum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c7412e69-e49a-4a3b-b7b1-13ea45a8b8f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 4]), torch.Size([4, 4]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt_config = GPT_Config(vocab_size=50257,cntx_len=4,d_model=4,n_head=4,n_layers=1)\n",
    "tt = GPT_torch(config=tt_config)\n",
    "x,y = get_batch('train',4,4)\n",
    "x.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ffa23509-eefa-46a5-b5f1-9e0582d1371d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 402,324 parameters\n",
      "Average Loss: 6.435782206058502\n",
      "Average Loss: 6.403727095127106\n",
      "Average Loss: 6.646209037303924\n",
      "Average Loss: 6.481482603549957\n",
      "Average Loss: 6.598695299625397\n",
      "Average Loss: 6.379622875452042\n",
      "Average Loss: 6.5467028558254245\n",
      "Average Loss: 6.651079334020615\n",
      "Average Loss: 6.392141599655151\n",
      "Average Loss: 6.587835516929626\n",
      "Final Average Loss: 6.649521796703339\n"
     ]
    }
   ],
   "source": [
    "lr_init = .1\n",
    "momentum = 0\n",
    "optimizer = torch.optim.AdamW(params=tt.parameters(),lr=lr_init)\n",
    "max_steps = 5000\n",
    "batch_size = 4\n",
    "eval_interval = 500\n",
    "step = 0\n",
    "warm_up_steps = 100\n",
    "# TODO: Estimate Losses\n",
    "\n",
    "def convert_readable(generated_output, enc = tiktoken.encoding_for_model('gpt2')):\n",
    "    return enc.decode_batch(generated_output.tolist())\n",
    "\n",
    "@torch.inference_mode()\n",
    "def eval_losses(iters):\n",
    "    losses = []\n",
    "    for _ in range(iters):\n",
    "        x,y = get_batch('valid',batch_size,tt_config.cntx_len)\n",
    "        loss,_ = tt(x,y)\n",
    "        losses.append(loss.item())\n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "    return avg_loss\n",
    "\n",
    "@torch.inference_mode()\n",
    "def update_grad(model, lr, momentum):\n",
    "    grad = sum(param.grad.norm().item() for param in model.parameters()) / sum(p.numel() for p in model.parameters())\n",
    "    return adaptive_momentum_scheduler(grad,lr,momentum)\n",
    "    \n",
    "\n",
    "tt.print_model_size()\n",
    "while True:        \n",
    "    step +=1\n",
    "    x,y = get_batch('train',batch_size,tt_config.cntx_len)\n",
    "    loss,_ = tt(x,y)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    lr, momentum = update_grad(tt,lr_init,momentum)\n",
    "    for pg in optimizer.param_groups:\n",
    "        pg['lr'] = lr\n",
    "    \n",
    "    # print(f'loss: {loss.item()}')\n",
    "    \n",
    "    \n",
    "    if step % eval_interval == 0:\n",
    "        avg_loss = eval_losses(200)\n",
    "        print(f'Average Loss: {avg_loss}')\n",
    "        \n",
    "    if step == max_steps:\n",
    "        print(f'Final Average Loss: {eval_losses(500)}')\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324e7f79-4f21-426b-a0ec-1bff49873086",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0590e9ed-2d46-48fc-a9b2-b230a4f31a91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
